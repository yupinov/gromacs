 \documentclass[11pt]{beamer}
\usetheme{Berlin}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{tikz}

\DeclareMathOperator\erf{erf}
\DeclareMathOperator\erfc{erfc}


\author[Aleksei Iupinov]{Aleksei Iupinov \\{\footnotesize Supervised by: Berk Hess}}
%\author{Aleksei Iupinov}
\title{Master thesis project: \\Particle mesh Ewald on a GPU}
\usepackage{subcaption}
%\setbeamercovered{transparent} 
\setbeamertemplate{navigation symbols}{} 
%\logo{} 
\institute{KTH Royal Institute of Technology} 
%\date{} 
%\subject{} 
\begin{document}
%\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\captionsetup[figure]{labelformat=empty}
\iffalse
asterisk
computebound solve? 
characteristics of all kernels?
B-spline formulae
steps
theta layout
pictures picctures pictures!
\fi
\begin{frame}	
\titlepage
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}

\section{Introduction}
\begin{frame}{Molecular dynamics simulations performance}
\begin{itemize}
\item typical MD use case: modelling large organic molecules (hundreds of thousands atoms)
\item scaling performance is important (time steps of $10^{-15}$ seconds, scale of processes is $10^{-6}$ -- $10^{-3}$ seconds) 
\item the general shift towards using GPU/accelerator highly-parallel hardware (a speed of single core stagnates $\implies$ increasing number of cores)
\item most computational time in MD simulations taken by electrostatic interactions
\item PME (particle mesh Ewald) method is used for computing a "long-range" component of electrostatic interactions
\end{itemize}
\end{frame}

\begin{frame}{Project background}
\begin{itemize}
\item open source MD software GROMACS 5.1
\item direct non-bonded particle pair interactions computed either on CPU or GPU (CUDA/OpenCL) 
\item PME only implemented for CPU
\item potential benefits of PME on GPU:
\begin{itemize}
\item more performance
\item most computation can be offloaded onto GPUs
\item avoiding CPU bottleneck on machines with lots of GPU power  
\end{itemize}
\item the task is an implementation of PME for a single GPU, using the NVIDIA CUDA library 
%mention cutoff
\end{itemize}
\end{frame}

\section{Problem}
\begin{frame}{Electrostatic interactions}
\begin{itemize}
\item particles coordinates $\bold{r}_1, \dotsc, \bold{r}_N$ and charges $q_1, \dotsc, q_N$ known, forces $\bold{f}_1, \dotsc, \bold{f}_N$ acting on particles to be computed 
\item electrostatic potential:
\[E(\bold{r}_1, \dotsc, \bold{r}_N) = \frac{1}{4 \pi \varepsilon_0}\sum\limits_{i < j}\frac{q_i q_j}{\lvert \bold{r}_i-\bold{r}_j\rvert}\]
\item forces to be derived:
\[\bold{f_i} = -\frac{\partial E}{\partial \bold{r_i}} \]
\item large $N$ and computational effort $O(N^2) \implies$ slow computation!
\end{itemize}
\end{frame}

\section{Method}
\begin{frame}{Ewald summation}
\begin{itemize}

\item the dependency of $E$ on $\bold{r}$ is $\frac{1}{\lvert \bold{r}\rvert }$ -- no convergence, no cut-off
\item Ewald summation: decomposition into direct and reciprocal space components, based on Gaussian error function
\item the direct space component converges within a certain cut-off
\item the reciprocal component converges in the Fourier space
\end{itemize}
%\noindent\makebox[\textwidth][c]{%
\begin{figure} 
\begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=1\textwidth]{pics/{cutoff0.7}.png}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=1\textwidth]{pics/cutoff2.png}
\end{minipage}
\caption{Parametrized decomposition}
\end{figure}
%}

%mention cutoff and balance

\end{frame}

\begin{frame}{Ewald sum conditions}
\begin{itemize}
\item Fourier component $\implies$ periodic boundary conditions (system is a unit cell tiled in all directions)
\item all charges interact with the periodic images of charges as well
\item charge neutrality of the unit cell is needed for convergence
\item additional correction component (charges acting on self, bonded interactions split from direct sum)
\item can be implemented in $O(N^\frac{3}{2})$

%mention single process

%lattice picture
\end{itemize}
\end{frame}

\begin{frame}{Ewald sum formulae}
\[E = E_{dir} + E_{rec} + E_{corr}\]
\[
E_{dir} = \frac{1}{2}\sum\limits_{\bold{n\in \mathbb{Z}^3}}^*\sum\limits_{i=1}^N\sum\limits_{j=1}^N\frac{q_i q_j \erfc(\beta\lvert\bold{r}_i - \bold{r}_j + \bold{n} \rvert)}{\lvert\bold{r}_i - \bold{r}_j + \bold{n} \rvert}
\]
\[
E_{rec}=\frac{1}{2\pi V}\sum\limits_{\bold{m} \in \{\mathbb{Z}^3 \setminus {0}\}}\frac{\exp(-\frac{\pi^2\bold{m}^2}{\beta^2})}{\bold{m}^2}S(\bold{m})S(\bold{-m})
\]
\[
 S(\bold{m}) = 
\sum\limits_{j=1}^Nq_j \exp(2\pi i\bold{m}\cdot\bold{r}_j)
\]
\[E_{corr} =  - \frac{\beta}{\sqrt{\pi}}\sum\limits_{i=1}^N q_i^2-\frac{1}{2}\sum\limits_{(i,j)\in M}\frac{q_i q_j \erf(\beta\lvert\bold{r}_i - \bold{r}_j\rvert)}{\lvert\bold{r}_i - \bold{r}_j\rvert}\]
%bold 0
\end{frame}

\begin{frame}{Particle mesh Ewald}
\begin{itemize}
% picture of charges
\item a way to approximate exponentials in the reciprocal sum 
\item interpolation onto a discrete, finite 3D grid
\item smooth PME: B-spline interpolation of order $p$, the potential function is smooth $\implies$ simple analytical derivation of forces
\item can be implemented in $O(N \log(N))$
% B-spline formulae
\end{itemize}
\end{frame}

\begin{frame}{PME stages}

\begin{enumerate}
\item calculate the B-spline interpolation values for each particle \\
(3 dimensions, $p$ values/derivatives in each, $p$ typically 4 or 5)
\item 
    spread the particle charges on a discrete 3D grid \\ ($p^3$ contributions per particle)
\item perform real-to-complex 3D FFT of the grid
\item calculate the reciprocal energy, transform the grid
\item perform complex-to-real 3D FFT of the grid
\item gather the particle forces from the grid and reduce \\ (again, $p^3$ contributions per particle)
\end{enumerate}
\end{frame}

%pictures?

\section{Implementation}

\begin{frame}{MD time step in GROMACS}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{pics/mdstep-orig.png}
    \caption{Timeline of MD step in a single-process GROMACS simulation}
    \label{fig:step-orig}
\end{figure}
\FloatBarrier
\end{frame}


\begin{frame}{MD time step in GROMACS, modified for PME GPU}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{pics/mdstep-gpu.png}
    \caption{Timeline of a modified MD step with PME GPU}
\end{figure}
\FloatBarrier
\end{frame}

%order 4!

\begin{frame}{Calculating the spline parameters}
\begin{itemize}
\item independent 3 dimensions of each particle's spline parameters $\implies$ 3 threads per particle
\item each dimension of a particle spline involves $p=4$ spline values/derivatives 
\item $p$ threads can perform redundant work for a single spline; \\each stores only $1$ value/derivative $\implies$ less global memory transactions
\item overall, $3 \cdot 4 = 12$ threads per particle 
\item united with the next stage - the charge spreading kernel \\ (to save on spline parameter accesses using shared memory) 
\end{itemize}
\end{frame}

\begin{frame}{Spreading the charges}
\begin{itemize}
\item the most important, memory-bound part of the run-time
\item charges accumulated onto the global grid in parallel $\implies$ clashes of atomic addition operations
\item for PME order $p=4$ each particle affects $4^3 = 64$ cells
\item a single thread works on one line (4 contributions)
\item a warp works exactly on 2 particles (16 threads per particle)
% picture
\item the grid is meant to be periodic
\item the implementation grid has an 'overlap' of $(p - 1)$ cells in each direction, is wrapped afterwards by a separate small kernel
\end{itemize}
\end{frame}

\begin{frame}{3D FFT}
\begin{itemize}
\item happens twice (real-to-complex before solving, complex-to-real after solving)
\item implemented as a simple call to CUDA cuFFT library
\item with multiple PME processes requires all-to-all communication of the decomposed grid
\item for this reason there exists an option of using the existing CPU FFT code  (MPI + calls to FFTW library) 
% overlap
\end{itemize}
\end{frame}

\begin{frame}{Solving in Fourier space}
\begin{itemize}
\item a small compute-bound kernel
\item performance dependent on the grid size
\item a single thread works on one grid cell
\item a thread block works on one/several grid lines, aligned by a warp size
\item energy reduction in shared memory per block, incrementing a global result atomically
\end{itemize}
\end{frame}

\begin{frame}{Gathering the forces}
\begin{itemize}
\item a less memory-intensive counterpart of spreading
\item the grid is unwrapped before the gather
\item has to perform force reduction for each particle in shared memory or registers
\item same layout and scheduling as spreading (4 contributions per thread, 16 threads per particle)
%\item typically 3 times as fast as spread on newer  
\end{itemize}
%layout picture
\end{frame}

\section{Results}
% why is this here
\begin{frame}{Results}
\begin{itemize}
\item PME implemented in CUDA for a single GPU
\item CPU PME used as reference for reciprocal and conserved energy values to ensure correctness
\item supports FFT both on GPU and CPU (should help with future multi-GPU parallelization) 
\item only interpolation order of 4 has been tried
\item supports separate PME process, PME grid size tuning, dynamic load balancing
% more about decomposition
\end{itemize}
\end{frame}

\begin{frame}{Individual stages performance}
\begin{itemize}
\item a water box with number of atoms varying from 960 to 1536000
\item a high end GPU (NVIDIA GTX TITAN X) 
\end{itemize}
\begin{figure}
    %1\centering
    \includegraphics[width=0.8\textwidth]{pics/kernels-noconcur-2.png}
    %\caption{Performance of the individual PME stages as a function of the number of the input particles on a loglog scale}
    \label{fig:kernels}
\end{figure}
\end{frame}

\begin{frame}{Single process CPU/GPU comparison, 134201 atoms}
\begin{itemize}
\item non-bonded interactions computed on GPU (NVIDIA GTX TITAN X)
\item PME computed either on a CPU (Intel Core i7-5960X) or on the same GPU
% threads varied
\end{itemize}
\FloatBarrier
\begin{figure} [h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/CPU_GPU_ADH_SINGLE.png}
\end{figure}
% superiority over weak CPUs
\FloatBarrier
\end{frame}

\begin{frame}{2 processes (1 PP / 1 PME) CPU/GPU comparison}
\begin{itemize}
\item non-bonded interactions computed on GPU (NVIDIA GTX TITAN X)
\item PME computed either on a CPU (Intel Core i7-5960X) or on the second GPU (NVIDIA Quadro M6000)
\end{itemize}
\FloatBarrier
% threads varied
\begin{figure} [h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/CPU_GPU_ADH.png}
    \label{fig:sepGPUNEW}
\end{figure}
\FloatBarrier
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
\begin{itemize}
\item the CUDA implementation for a single GPU -- a foundation for further work
\item should be easily extensible for scaling over multiple GPUs
(compatible with the CPU FFT code supporting the Fourier grid decomposition)
\item should also be easily extensible for interpolation orders other than 4
(higher orders $\implies$ coarser grid $\implies$ less communication)
\item supports a separate PME process with direct space decomposed over multiple PP processes, already viable for a machine with several NVIDIA GPUs 
\item hopefully integrated into GROMACS in a few months
\end{itemize}
\end{frame}

%\begin{frame}[plain]
%      Thank you for listening! Questions?
%\end{frame}

%references, github link

%PP, NB, other terms

% where does the grid size come from?x

\end{document}