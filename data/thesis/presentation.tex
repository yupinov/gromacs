\documentclass[11pt]{beamer}
\usetheme{Berlin}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{tikz}
\author{Aleksei Iupinov}
\title{Particle mesh Ewald on a GPU}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
\institute{KTH Royal Institute of Technology} 
%\date{} 
%\subject{} 
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Molecular dynamics simulation performance}
\begin{itemize}
\item modelling large organic molecules (hundreds of thousands atoms)
\item scaling performance is important (timesteps of $10^{-15}$ seconds, processes of $10^{-6}$ -- $10^{-3}$ seconds) 
\item the balance shifting towards GPU/accelerator hardware (core speed limited, increasing number of cores instead)
\item most computational time -- electrostatic interactions
\item PME used for computing long-range component
\end{itemize}
\end{frame}

\section{Problem}
\begin{frame}{Electrostatic interactions}
\begin{itemize}
\item particles coordinates $r_1 .. r_N$ and charges $q_1 .. q_N$ known, forces acting on particles to be computed 
\item electrostatic potential:
\[E = \frac{1}{4 \pi \varepsilon_0}\sum\limits_{i < j}\frac{q_i q_j}{\lvert r_i-r_j\rvert}\]
% force derivation
\item large $N$ and computational effort $O(N^2) \implies$ slow computation!
\end{itemize}
\end{frame}

\section{Method}
\begin{frame}{Ewald sum}
\begin{itemize}
\item interaction dependency is $\frac{1}{r}$ - not converging
% graph of decompose
% not converging?
\item Ewald sum: decomposition into direct and reciprocal space components

\FloatBarrier
\begin{figure} 
    \includegraphics[width=0.2\textwidth]{pics/cutoff1.png}
\end{figure}
\begin{figure} 
    \includegraphics[width=0.2\textwidth]{pics/cutoff2.png}
\end{figure}
\FloatBarrier
% formulas
\item the direct space component converges before a certain cut-off
\item the reciprocal component converges well in the Fourier space
\end{itemize}
\end{frame}

\begin{frame}{Ewald sum}
\begin{itemize}
\item Fourier component $\implies$ periodic boundary conditions (system is a unit cell tiled in all directions)
\item charge neutrality of the unit cell is needed for convergence
\item can be implemented in $O(N^\frac{3}{2})$
%\item manipulating the cut-off can change the balance

%\item method originally used in crystallography
%lattice picture
%formulas
\end{itemize}
\end{frame}

\begin{frame}{Particle mesh Ewald}
\begin{itemize}
%picture of charges
\item a way to approximate exponentials in the reciprocal sum 
\item interpolation onto a discrete 3D grid
\item smooth PME: B-Spline interpolation, the potential function is smooth $\implies$ simple analytical derivation of forces
\item can be implemented in $O(N \log(N))$
\end{itemize}
\end{frame}

\begin{frame}{PME stages}
\begin{enumerate}
\item Calculate the B-spline interpolation values for all particles (equation \eqref{BSpline}).
\item Spread the particle charges on a discrete 3D grid (using the spline values), calculating the array $Q$ (equation \eqref{Qspread}).
\item Perform three-dimensional real-to-complex FFT of the grid.
\item Calculate the reciprocal energy contribution(equation \eqref{Erecfull}), transform the grid.
\item Perform three-dimensional complex-to-real FFT of the convoluted grid.
\item Gather the particle forces from the grid (using the spline values) (equation \eqref{Fgather}).
\end{enumerate}
\end{frame}

%pictures?

\section{Implementation}
\begin{frame}{Implementation (or still problem)}
\begin{itemize}
\item open source MD software GROMACS 5.1
\item short-range particle pair interactions computed either on CPU or GPU (CUDA/OpenCL)
\item PME only implemented for CPU
\item PME GPU implementation can mean: more performance, more load balancing, more GPU hardware utilisation, less CPU bottleneck 
%mention cutoff
\end{itemize}
\end{frame}

\begin{frame}{PME GPU implementation potential benefits}
\begin{itemize}
\item more performance
\item more possibilities for load balancing
\item nearly all computation can be offloaded onto GPUs
\item avoiding CPU bottleneck on machines with prevailing GPU power  
\end{itemize}
\end{frame}

\begin{frame}{...}
\end{frame}

\begin{frame}{weird FFT}
\end{frame}

\section{Results}
\begin{frame}{Results}
\begin{itemize}
\item implemented for a single GPU (should be extensible...)
\item supports GPU and CPU FFT, which is the main parallelization barrier 
\item only interpolation order of 4 (again, not a big limitation, the core is there)
\item works in a single PME/PP process
\item or as a single PME process with several PP ranks
\item viable for small simulations on machines with several GPUs
\item hopefully parallelized
\item load balancing works
\item integrated soon, accessible here
% mroe about decomposition
% mroe about tuning
% more about B-splines
% references?
\end{itemize}
\end{frame}



\begin{frame}{Single process CPU/GPU comparison}
\begin{itemize}
\item short-range computed on GPU
\item PME computed either on a CPU or on NVIDIA Quadro M6000 GPU
\end{itemize}
%described comnditions
\FloatBarrier
\begin{figure} [h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/CPU_GPU_ADH_SINGLE.png}
\end{figure}
\FloatBarrier
\end{frame}


%{This comparison demonstrates not only a noticeable parallelization speed-up of a separate rank PME on a high-end GPU, but also a possibility of shifting the PME workload onto a GPU from the CPU. It is possible to enhance the load balancing algorithm of GROMACS to perform the switching between GPU and CPU PME dynamically as required.%}

For the next comparison the same machine is used, but a simulation consisting of 2 separate processes is ran: 1 PP and 1 PME process. The PP process performs the short-range computations on the high-end NVIDIA GPU GTX Titan X, as before.    
The PME process is ran in one case on the CPU (Intel Core i7-5960X), and in another case on the second NVIDIA GPU Quadro M6000. The number of the threads used by the processes is varied from 1 to 8.
 
The resulting simulation performance comparison between the separate CPU and GPU PME ranks is presented at the figure \ref{fig:sepGPUNEW}.
\begin{frame}{Separate process CPU/GPU comparison}
\FloatBarrier
\begin{figure} [h!]
    \centering
    \includegraphics[width=1\textwidth]{pics/CPU_GPU_ADH.png}
    \label{fig:sepGPUNEW}
\end{figure}
\FloatBarrier
\end{frame}

\section{Conclusion}
\begin{frame}{Conclusion}
\end{frame}

\begin{frame}[plain]
      Thank you for listening!
\end{frame}

%references











\end{document}